# -*- coding: utf-8 -*-
"""Quant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CSb42KZwnDATgWHr28lOpBPJEOX0Uzi1
"""

!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install yfinance
!pip install scikit-learn
!pip install PyWavelet
!pip install gdown

!pip install xgboost
!pip install deap

# Installa ta-lib python 11
url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'
!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1
!pip install conda-package-handling
!wget https://anaconda.org/conda-forge/ta-lib/0.5.1/download/linux-64/ta-lib-0.5.1-py311h9ecbd09_0.conda
!cph x ta-lib-0.5.1-py311h9ecbd09_0.conda
!mv ./ta-lib-0.5.1-py311h9ecbd09_0/lib/python3.11/site-packages/talib /usr/local/lib/python3.11/dist-packages/

# Scarica il file TechnicalAnalysis.py nel runtime di colab (usa se dà problemi a importare TechnicalAnalysis)
!gdown https://drive.google.com/file/d/1rcClcvH7wtt5wrJB0GZWQ9T77aQlQiWV/view?usp=drive_link --fuzzy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf

# TechnicalAnalysis implementa TA-lib per il calcolo degli indicatori scelti
import TechnicalAnalysis as ta

data_save_name = "US500_2022.parquet"
ta_save_name = "TA_US500_2022.parquet"

# # Scarico i dati, formatto le colonne per i passaggi successivi e salvo su file il df per evitare di dover scaricare i dati ogni volta
# # Usare questa cella ogni giorno per aggiornare i dati salvati
data = ta.yf_download_data("SPY", "2021-01-25") # Utilizzo i dati dal 2022 per velocizzare le operazioni (solo esempio)
data = ta.data_format(data)
data.to_parquet(data_save_name)

# Utilizzo questa cella per caricare i dati che ho salvato (se non voglio riscaricarli da yf)
data = pd.read_parquet(data_save_name)



data.shape

# Calcolo tutti gli indicatori
# Tutti tranne OBV hanno dei parametri da impostare
# La chiamata senza parametri utilizza i default di TA-lib
# I parametri sono sostanzialmente periodi di diverse cose, in base all'indicatore calcolato
analysis = ta.TechnicalAnalysis(data)
analysis.rsi()
analysis.macd()
analysis.ppo()
analysis.adx()
analysis.momentum()
analysis.cci()
analysis.roc()
analysis.stochastic()
analysis.williams_r()
analysis.sma(20)
analysis.sma(50)
analysis.sma(100)
analysis.ema(20)
analysis.ema(50)
analysis.ema(100)
analysis.bollinger_bands()
analysis.parabolic_sar()
analysis.obv()
analysis.chaikin_oscillator()
analysis.mfi()
analysis.atr()
data = analysis.data

# Se il calcolo richiede molto tempo conviene salvare in un nuovo file
data.to_parquet(ta_save_name)

# Uso se ho salvato già l'analisi tecnica
data = pd.read_parquet(ta_save_name)

# Nel nome della colonna è possibile vedere in ordine i parametri con cui l'indicatore è stato calcolato
data

#scarico il CSV sul mio computer
from google.colab import files
data.to_csv('data.csv', index=False) # index=False per evitare che l'indice venga salvato nel file
files.download('data.csv')

#salva data in un csv sulla cartella
data.dropna()
data.to_csv('data.csv')

#write close as the next day open

analysis = ta.TechnicalAnalysis(data)
analysis.rsi()
analysis.macd()
analysis.ppo()
analysis.adx()
analysis.momentum()
analysis.cci()
analysis.roc()
analysis.stochastic()
analysis.williams_r()
analysis.sma(20)
analysis.sma(50)
analysis.sma(100)
analysis.ema(20)
analysis.ema(50)
analysis.ema(100)
analysis.bollinger_bands()
analysis.parabolic_sar()
analysis.obv()
analysis.chaikin_oscillator()
analysis.mfi()
analysis.atr()
data = analysis.data
data.fillna(method='bfill', inplace=True)

data

# #scarico il CSV sul mio computer
# from google.colab import files
# data.to_csv('data.csv', index=False) # index=False per evitare che l'indice venga salvato nel file
# files.download('data.csv')

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

train, temp = train_test_split(data, test_size=0.4, random_state=0)
val, test = train_test_split(temp, test_size=0.5, random_state=0)

scaler = MinMaxScaler()
scaler.fit(data)
train_scaled = scaler.transform(train)
val_scaled = scaler.transform(val)
test_scaled = scaler.transform(test)


data["returns"] = data['Close']-data['Open']
data["target"] = data["returns"].apply(lambda x: 1 if x > 0 else 0)
target = data["target"]
data.drop(["returns"], axis=1, inplace=True)

#aggiungi i corrisoettivi target a train val e test
train_target = target[:len(train)]
val_target = target[len(train):len(train)+len(val)]
test_target = target[len(train)+len(val):]

#any nan?
data.isnull().values.any()

from sklearn.decomposition import PCA
pca1 = PCA()
pca2 = PCA()
pca3 = PCA()
pca1.fit(train_scaled)
pca2.fit(val_scaled)
pca3.fit(test_scaled)
train_pca = pca1.transform(train_scaled)
val_pca = pca2.transform(val_scaled)
test_pca = pca3.transform(test_scaled)

plt.plot(np.cumsum(pca1.explained_variance_ratio_))
plt.plot(np.cumsum(pca2.explained_variance_ratio_))
plt.plot(np.cumsum(pca3.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.grid()
plt.axhline(y=0.95, color='r', linestyle='-') #prendi 95% di varianza spiegata come target, accade al componente 5(che sarebbe il 6 in pythonese)
plt.show()

plt.plot(train_pca[:,0:5]) #per visualizzazione e basta, ignora questo grafico



#mi estraggo solamente i componenti interessanti
train_pca=train_pca[:,0:5]
val_pca=val_pca[:,0:5]
test_pca=test_pca[:,0:5]

train_pca

!pip install PyWavelets

import pywt
#sci-kit image
from skimage import data
from skimage import filters

#daubechies wavelet of order 3
wavelet = 'db3'
#compute the wavelet coefficients
coeffs_train_2 = pywt.wavedec(train_pca, wavelet, level=5)

coeffs_train=np.array(coeffs_train_2)
#coeffs_train[i,:,j] dove i è il numero del componente principale e va da 0 a 5(6 elementi), : è la serie al completo, j va da 0 a 4 dove 0 sono i coefficienti di approssimazione, 1-4 quelli di dettaglio

coeff_approx_train=coeffs_train[:,:,0]
coeff_detail_train=coeffs_train[:,:,1:]

def bayes_shrink_denoise(detail_coeffs):
    denoised_coeffs = np.zeros_like(detail_coeffs)

    for level in range(detail_coeffs.shape[1]):
        for feature in range(detail_coeffs.shape[2]):
            coeff = detail_coeffs[:, level, feature]

            sigma_squared = np.median(np.abs(coeff)) / 0.6745 #il perché non lo sa nessuno, prassi comune a detta di chat, da riverificare
            sigma_squared = sigma_squared ** 2

            signal_variance = np.var(coeff)
            sigma_s = np.sqrt(max(signal_variance - sigma_squared, 0))

            # Calcolo della soglia BayesShrink
            if sigma_s != 0:
                threshold = sigma_squared / sigma_s
            else:
                threshold = np.inf

            # Soft thresholding perché nel paper dicono cosi'
            denoised_coeffs[:, level, feature] = pywt.threshold(coeff, threshold, mode='soft')

    return denoised_coeffs

# Applica BayesShrink ai coefficienti di dettaglio del training set
denoised_detail_train = bayes_shrink_denoise(coeff_detail_train)



def bayes_shrink_denoise_2(coeff_approx_train):

    sigma_squared = np.median(np.abs(coeff_approx_train)) / 0.6745 #idem come sopra
    sigma_squared = sigma_squared ** 2
    signal_variance = np.var(coeff_approx_train)
    sigma_s = np.sqrt(max(signal_variance - sigma_squared, 0))

    threshold = sigma_squared / sigma_s
    denoised_approx_train = pywt.threshold(coeff_approx_train, threshold, mode='soft')

    return denoised_approx_train

denoised_approx_train = bayes_shrink_denoise_2(coeff_approx_train)

#trasfrma i coefficienti approssimati e dettagliati in un unico array
denoised_coeffs_train = np.zeros((denoised_approx_train.shape[0], denoised_detail_train.shape[1], denoised_detail_train.shape[2] + 1))
denoised_coeffs_train[:, :, 0] = denoised_approx_train
denoised_coeffs_train[:, :, 1:] = denoised_detail_train

#convertto l'array in una lista di 6 cosi che è come l'originale
denoised_coeffs_train = denoised_coeffs_train.tolist()
denoised_coeffs_train = [np.array(i) for i in denoised_coeffs_train]


#ricompongo i dati (se non è un list di array non funziona... tecnicamente lo split dei appr e det non andava fatto come ho fatto io, credo ci fosse un metodo implicito nel pacchetto ma non mi era chiaro)
denoised_train = pywt.waverec(denoised_coeffs_train, wavelet)

plt.plot(denoised_train[:,0], label='Denoised')
plt.plot(train_pca[:,0], label='PCA', color='r')

#Questo grafico mostra la diversa intensità del segnale a diverse frequenze. Denoisizzarlo funziona e si vede


from scipy.signal import welch

frequencies, power = welch(train_pca[:,0], fs=10, nperseg=1000)
frequencies_denoised, power_denoised = welch(denoised_train[:,0], fs=10, nperseg=1000)

plt.figure()
plt.plot(frequencies, power, label='Original')
plt.plot(frequencies_denoised, power_denoised, label='Denoised', color='r')

"""STESSA PROCEDURA MA PER VALIDATION SET (SERVE A TROVARE IL NUEMRO DI PARAMETRI
NEL XGB)
"""

for i in range(len(val_pca)):
    # combina i dati di training e validation un gionro alla volta
    combined_data = np.vstack((train_pca, val_pca[:i+1]))

    #coefficienti calcolati un giorno alla volta
    coeffs_val_2 = pywt.wavedec(combined_data, wavelet, level=5)
    coeffs_val = np.array(coeffs_val_2)
    coeff_approx_val = coeffs_val[:, :, 0]
    coeff_detail_val = coeffs_val[:, :, 1:]

    #baYES SHIRNK
    denoised_detail_val = bayes_shrink_denoise(coeff_detail_val)
    denoised_approx_val = bayes_shrink_denoise_2(coeff_approx_val)

    # Ricostruiscw il coefficienti denoised
    denoised_coeffs_val = np.zeros((denoised_approx_val.shape[0], denoised_detail_val.shape[1], denoised_detail_val.shape[2] + 1))
    denoised_coeffs_val[:, :, 0] = denoised_approx_val
    denoised_coeffs_val[:, :, 1:] = denoised_detail_val
    denoised_coeffs_val = denoised_coeffs_val.tolist()
    denoised_coeffs_val = [np.array(i) for i in denoised_coeffs_val]

    # Ricostruisce il segnale denoised
    denoised_val = pywt.waverec(denoised_coeffs_val, wavelet)

#ruomovi da denoised val la lunghezza di train_pca e confronta con val_pca (dovrebbero avere la stessa lunghezza ora)
denoised_val=denoised_val[len(train_pca):]

plt.plot(denoised_val[:,0], label='Denoised')
plt.plot(val_pca[:,0], label='Original', color='r')

"""IDEM CON PATATE CON TEST

"""

for i in range(len(test_pca)):
    #qua inserisco pure val nella procedura
    combined_data = np.vstack((train_pca, val_pca, test_pca[:i+1]))

    coeffs_test_2 = pywt.wavedec(combined_data, wavelet, level=5) #questo _2 serve per non sovrascrivere i file in modo da poter sempre controllare l'oggetto originale che è una cosa strana
    coeffs_test = np.array(coeffs_test_2)
    coeff_approx_test = coeffs_test[:, :, 0]
    coeff_detail_test = coeffs_test[:, :, 1:]
    denoised_detail_test = bayes_shrink_denoise(coeff_detail_test)
    denoised_approx_test = bayes_shrink_denoise_2(coeff_approx_test)
    denoised_coeffs_test = np.zeros((denoised_approx_test.shape[0], denoised_detail_test.shape[1], denoised_detail_test.shape[2] + 1))
    denoised_coeffs_test[:, :, 0] = denoised_approx_test
    denoised_coeffs_test[:, :, 1:] = denoised_detail_test
    denoised_coeffs_test = denoised_coeffs_test.tolist()
    denoised_coeffs_test = [np.array(i) for i in denoised_coeffs_test]

    denoised_test = pywt.waverec(denoised_coeffs_test, wavelet)

denoised_test=denoised_test[len(train_pca)+len(val_pca):]

plt.plot(denoised_test[:,4], label='Denoised')
plt.plot(test_pca[:,4], label='Original', color='r')

import xgboost as xgb
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms

returns_norm_train = train["Close"].pct_change()
returns_norm_val = val["Close"].pct_change()
returns_norm_test = test["Close"].pct_change()

def genetic_algorithm_optimization(denoised_train, train_target, returns_norm_train):

    creator.create("FitnessMax", base.Fitness, weights=(1.0, 1.0))  # Maximize both accuracy and Sharpe ratio
    creator.create("Individual", list, fitness=creator.FitnessMax)
    # Initialize the toolbox
    toolbox = base.Toolbox()

    # Register the attributes and individual creation functions
    toolbox.register("attr_float", np.random.uniform, 0, 1)  # Uniform distribution between 0 and 1
    toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=4)  # n=4 hyperparameters
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)

    def evaluate_individual(individual):
        learning_rate, max_depth, min_child_weight, subsample = individual

        # Ensure the learning_rate is positive and within the valid range
        learning_rate = max(learning_rate, 0.0001)  # Make sure learning rate is not too small (negative or zero)

        # Ensure max_depth and min_child_weight are positive integers
        max_depth = max(1, int(max_depth * 10))  # Scale and make sure it is at least 1
        min_child_weight = max(1, int(min_child_weight * 10))  # Scale and make sure it is at least 1

        # Ensure subsample is between 0 and 1
        subsample = np.clip(subsample, 0.5, 1.0)  # Constrain subsample to be between 0.5 and 1.0

        # Initialize XGBoost classifier with these hyperparameters
        model = XGBClassifier(
            objective='binary:logistic',
            learning_rate=learning_rate,
            max_depth=max_depth,
            min_child_weight=min_child_weight,
            subsample=subsample,
            eval_metric='auc'
        )

        # Train the model and calculate the score
        model.fit(denoised_train, train_target)
        score = model.score(denoised_train, train_target)  # Accuracy
        excess_ret = returns_norm_train
        sharpe_ratio = excess_ret.mean() / returns_norm_train.std() if returns_norm_train.std() > 0 else 0  # Simple Sharpe ratio calculation

        return score, sharpe_ratio  # Two objectives: Accuracy and Sharpe ratio

    # Register the genetic algorithm operations
    toolbox.register("mate", tools.cxTwoPoint)
    toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)  # Mutation
    toolbox.register("select", tools.selNSGA2)  # Multi-objective selection (NSGA-II)
    toolbox.register("evaluate", evaluate_individual)

    # Create the initial population
    pop = toolbox.population(n=128)  # 50 individuals

    # Run the evolutionary algorithm
    final_pop = algorithms.eaMuPlusLambda(pop, toolbox, mu=58, lambda_=70, cxpb=0.5, mutpb=0.2, ngen=100, verbose=True)

    # Extract the best solution based on fitness values (using the accuracy and Sharpe ratio)
    best_solution_individual = tools.selBest(final_pop[0], 1)[0]  # Select the best individual
    best_solution = {
        "learning_rate": best_solution_individual[0],
        "max_depth": int(best_solution_individual[1] * 10),  # Scaling factor
        "min_child_weight": max(1, int(np.clip(best_solution_individual[2], 0, 1) * 10)),
        "subsample": best_solution_individual[3]
    }

    return best_solution

best_solution = genetic_algorithm_optimization(denoised_train, train_target, returns_norm_train)
print(best_solution)

final_model = XGBClassifier(
    objective='binary:logistic',
    learning_rate=best_solution["learning_rate"],
    max_depth=best_solution["max_depth"],
    min_child_weight=best_solution["min_child_weight"],
    subsample=best_solution["subsample"],
    eval_metric='auc'
)
final_model.fit(denoised_train, train_target)

validation_accuracy = final_model.score(denoised_val, val_target)
validation_predictions = final_model.predict(denoised_val)
validation_returns_norm =  returns_norm_val
validation_sharpe = validation_returns_norm.mean() / validation_returns_norm.std()
print(validation_accuracy, validation_sharpe)

test_accuracy = final_model.score(denoised_test, test_target)
test_predictions = final_model.predict(denoised_test)
test_returns_norm = returns_norm_test
test_sharpe = test_returns_norm.mean() / test_returns_norm.std()
print(f"Test Accuracy: {test_accuracy}, Sharpe Ratio: {test_sharpe}")

print(test_predictions[-1])

test_predictions

test_target